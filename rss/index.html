<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Karn Wong's Blog]]></title><description><![CDATA[Karn Wong's Blog]]></description><link>https://blog.karnwong.me/</link><image><url>https://blog.karnwong.me/favicon.png</url><title>Karn Wong&apos;s Blog</title><link>https://blog.karnwong.me/</link></image><generator>Ghost 4.11</generator><lastBuildDate>Sat, 28 Aug 2021 09:09:52 GMT</lastBuildDate><atom:link href="https://blog.karnwong.me/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Moved!]]></title><description><![CDATA[<p>Moved to <a href="https://karnwong.me/posts">https://karnwong.me/posts</a></p>]]></description><link>https://blog.karnwong.me/moved/</link><guid isPermaLink="false">6129fce6736ac000011f685a</guid><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sat, 28 Aug 2021 09:08:25 GMT</pubDate><content:encoded><![CDATA[<p>Moved to <a href="https://karnwong.me/posts">https://karnwong.me/posts</a></p>]]></content:encoded></item><item><title><![CDATA[Python venv management]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>When you create a project in python, you should create <code>requirements.txt</code> to specify dependencies, so other people can have the same environment when using your project.</p>
<p>However, if you don&#x2019;t specify module versions in <code>requirements.txt</code>, you could end up with people using the wrong module version,</p>]]></description><link>https://blog.karnwong.me/python-venv-management/</link><guid isPermaLink="false">60dec6568838f700014db16a</guid><category><![CDATA[python]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Fri, 02 Jul 2021 07:59:00 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>When you create a project in python, you should create <code>requirements.txt</code> to specify dependencies, so other people can have the same environment when using your project.</p>
<p>However, if you don&#x2019;t specify module versions in <code>requirements.txt</code>, you could end up with people using the wrong module version, where some APIs can be deprecated or have different behaviors than older versions.</p>
<p>Another issue is that maybe you&#x2019;re working on a few python projects, each uses different python versions (eg. projectA uses python3.6, projectB uses python3.9, etc).</p>
<p>Enters <code>pyenv</code> and <code>pipenv</code> (I will discuss about <code>poetry</code> later), where you can easily switch python versions, and have different environment (with python version locking) for projects you&#x2019;re working on.</p>
<h1 id="installing-pyenv">Installing pyenv</h1>
<p>Follow instructions <a href="https://github.com/pyenv/pyenv">here</a>. For windows, use <a href="https://github.com/pyenv-win/pyenv-win">this</a>.</p>
<h2 id="useful-commands">Useful commands</h2>
<pre><code class="language-bash"># list available python versions
pyenv install --list

# install specific version
pyenv install 3.8.0

# list installed versions
pyenv versions

# activate new env
pyenv shell 3.8.0 # support multiple version

# config venv
pyenv virtualenv 3.8.0 my-data-project

# set env per folder/project
pyenv local my-data-project
</code></pre>
<h1 id="installing-pipenv">Installing <a href="https://github.com/pypa/pipenv">pipenv</a></h1>
<p>Notes: make sue <code>pyenv</code> is installed, and remove <code>anaconda / miconda</code> &amp; <code>python3</code> installed via official installer from your system. Then run:</p>
<pre><code class="language-bash">$ pip install pipenv

# run this command every time pip installs a .exe
$ pyenv rehash
</code></pre>
<h2 id="pipenv-workflow">pipenv workflow</h2>
<pre><code class="language-bash">pipenv --python 3.7

# install a specific module
pipenv install jupyterlab==2.2.9

# install from existing requirements.txt or from Pipfile definition
pipenv install

# remove venv
pipenv --rm

# running inside venv
pipenv run jupyter lab
pipenv run python main.py # is equivalent to `pipenv shell &amp;&amp; python3 main.py`
</code></pre>
<h2 id="windows-only">Windows only</h2>
<pre><code class="language-bash">$ pyenv install 3.7.7 # see Pipfile for required python version
$ pyenv local 3.7.7 # IMPORTANT. global / shell doesn&apos;t work with pipenv
$ pyenv rehash
$ pip install pipenv # done once per pyenv python version
$ pyenv rehash
$ pipenv --python 3.7
$ pipenv install
$ pipenv run python tokenization_sandbox.py
</code></pre>
<h1 id="notes">Notes</h1>
<ul>
<li>On linux/mac, do not use system python. OS updates would mean python version upgrade, in turn making all your installed modules gone. Use python installed via pyenv instead.</li>
<li>On windows, start fresh with pyenv.</li>
<li>Do not use anaconda distribution. It does too much background magic that can make things harder to manage environment property. In addition, venv definition from anaconda is often doesn&#x2019;t work cross-platform (eg. venv def from windows wouldn&#x2019;t work on mac due to different wheel binary versions).</li>
<li>Always create venv via pipenv per each project. Although you can have a playground venv via pyenv, so you can shell into it and do a quick analysis / scripting on an adhoc basis.</li>
<li>I heard good things about <a href="https://github.com/python-poetry/poetry">poetry</a> but it doesn&#x2019;t integrate with <code>pyenv</code> natively. It would work if you use it to publish python modules, since it simplifies a lot of processes.
<ul>
<li>poetry also picks up the wrong python version from pyenv. And if you sync python version via pyenv, it has to be the same python version across all OSes, including minor version. pipenv doesn&#x2019;t have this restriction, and it also picks up the correct python version from pyenv by default (via <code>pipenv --python 3.8</code>).</li>
</ul>
</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Don't write large table to postgres with pandas]]></title><description><![CDATA[<p>We have a few tables where the data size is &gt; 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&apos;t use columnar database).</p><p>I want to explore whether there&</p>]]></description><link>https://blog.karnwong.me/dont-write-to-warehouse-with-pandas/</link><guid isPermaLink="false">60d878818838f700014db12f</guid><category><![CDATA[data engineering]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sun, 27 Jun 2021 13:19:10 GMT</pubDate><content:encoded><![CDATA[<p>We have a few tables where the data size is &gt; 3GB (in parquet, so around 10 GB uncompressed). Loading it into postgres takes an hour. (Most of our tables are pretty small, hence the reason why we don&apos;t use columnar database).</p><p>I want to explore whether there&apos;s a faster way or not. The conclusion is writing to postgres with spark seems to be fastest, given we can&apos;t use <code>COPY</code> since our data contain free text, which means it would make CSV parsing impossible.</p><p>I also found out that the write performance from pandas to postgres is excruciatingly slow because:</p><ul><li>It first decompresses the data in-memory. For a 30MB parquet (around 100MB uncompressed) it used more than 20GB of RAM (I killed the task before it finishes, since by this time the RAM usage is climbing up)</li><li>But even with reading plain JSON line in pandas with chunksize and use <code>to_sql</code> with <code>multi</code> option, it&apos;s still very slow.</li></ul><p>In contrast, writing the said 30MB parquet file to postgres takes only 1 minute.</p><p>Big data is fun, said data scientists &#x1F9EA; (until they run out of RAM &#x1F606;)</p>]]></content:encoded></item><item><title><![CDATA[Data engineering toolset (that I use) glossary]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h1 id="big-data">Big data</h1>
<ul>
<li>Spark: Map-reduce framework for dealing with big data, especially for data that doesn&apos;t fit into memory. Utilizes parallelization.</li>
</ul>
<h1 id="cloud">Cloud</h1>
<ul>
<li>AWS: Cloud platform for many tools used in software engineering.</li>
<li>AWS Fargate: A task launch mode for <code>ECS task</code>, where it automatically shuts down once a container</li></ul>]]></description><link>https://blog.karnwong.me/data-engineering-toolset-glossary/</link><guid isPermaLink="false">60ba55438838f700014db0eb</guid><category><![CDATA[data engineering]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Fri, 04 Jun 2021 16:57:04 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h1 id="big-data">Big data</h1>
<ul>
<li>Spark: Map-reduce framework for dealing with big data, especially for data that doesn&apos;t fit into memory. Utilizes parallelization.</li>
</ul>
<h1 id="cloud">Cloud</h1>
<ul>
<li>AWS: Cloud platform for many tools used in software engineering.</li>
<li>AWS Fargate: A task launch mode for <code>ECS task</code>, where it automatically shuts down once a container exits. With EC2 launch mode, you&apos;ll have to turn off the machine yourself.</li>
<li>AWS Lambda: Serverless function, can be used with docker image too. Can also hook this with API gateway to make it act as API endpoint.</li>
<li>AWS RDS: Managed databases from AWS.</li>
<li>ECS Task: Cron-like schedule for a task. Essentially at specified time, it runs a predefined docker image (you should configure your <code>entrypoint.sh</code> accordingly).</li>
</ul>
<h1 id="data">Data</h1>
<ul>
<li>Parquet: Columnar data blob format, very efficient due to column-based compression with schema definition baked in.</li>
</ul>
<h1 id="data-engineering">Data engineering</h1>
<ul>
<li>Dagster: Task orchestration framework with built-in pipelines validatioin.</li>
<li>ETL: Stands for extract-transform-load. Essentially it means &quot;moving data from A to B, with optional data wrangling in the middle.&quot;</li>
</ul>
<h1 id="data-science">Data science</h1>
<ul>
<li>NLP: Using machine (computer) to work on human languages. For instance, analyze whether a message is positive or negative.</li>
</ul>
<h1 id="data-wrangling">Data wrangling</h1>
<ul>
<li>Pandas: Dataframe wrangler, think of programmable Excel.</li>
</ul>
<h1 id="database">Database</h1>
<ul>
<li>Postgres: RMDBS with good performance.</li>
</ul>
<h1 id="dataops">DataOps</h1>
<ul>
<li>Great expectations: A framework for data validation.</li>
</ul>
<h1 id="devops">DevOps</h1>
<ul>
<li>Docker: Virtualization via containers.</li>
<li>Git: Version control.</li>
<li>Kubernetes: Container orchestration system.</li>
<li>Terraform: Infrastructure as code tool, essentially you use it to store a blueprint for your infra setup. If you were to move to another account, you can re-conjure existing infra with one command. This makes editing infra config easier too, since it automatically cleans up / update config automatically.</li>
</ul>
<h1 id="gis">GIS</h1>
<ul>
<li>PostGIS: GIS extension for Postgres.</li>
</ul>
<h1 id="mlops">MLOps</h1>
<ul>
<li>MLflow: A framework to track model parameters and output. Can also store model artifact as well.</li>
</ul>
<h1 id="notebook">Notebook</h1>
<ul>
<li>Jupyter: Python notebook, used for exploring solutions before converting it to .py.</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Automatic scrapy deployment with GitHub actions]]></title><description><![CDATA[<p>Repo <a href="https://github.com/kahnwong/scrapy-deploy-gh-actions">here</a></p><p>Scrapy is a nice framework for web scraping. But like all local development processes, some settings / configs are disabled.</p><p>This wouldn&apos;t pose an issue, but to deploy a scrapy project to zyte (a hosted scrapy platform) you need to run <code>shub deploy</code>, and if you run</p>]]></description><link>https://blog.karnwong.me/automatic-scrapy-deployment-with-github-actions/</link><guid isPermaLink="false">60b7965b8838f700014db096</guid><category><![CDATA[infra]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Wed, 02 Jun 2021 14:48:34 GMT</pubDate><content:encoded><![CDATA[<p>Repo <a href="https://github.com/kahnwong/scrapy-deploy-gh-actions">here</a></p><p>Scrapy is a nice framework for web scraping. But like all local development processes, some settings / configs are disabled.</p><p>This wouldn&apos;t pose an issue, but to deploy a scrapy project to zyte (a hosted scrapy platform) you need to run <code>shub deploy</code>, and if you run it and forget to reset the config back to prod settings, a Titan may devour your home.</p><p>You can set auto deployment from github via the UI in zyte, but it only works with github only. Plus if you want to run some extra tests during CI/CD you&apos;re out of luck. So here&apos;s how to set up CI/CD to deploy automatically:</p><p><strong>Note</strong>: I would assume that you have your scrapy project set up already.</p><h2 id="create-scrapinghubyml-add-repo-secrets">Create scrapinghub.yml + add repo secrets</h2><pre><code class="language-yaml">project: ${PROJECT_ID}

requirements:
  file: requirements.txt

stack: scrapy:${YOUR_SCRAPY_VERSION_IN_PIPFILE}
apikey: null</code></pre><p>Notice that <code>apikey</code> is left blank. This is because it&apos;s considered a good practice to not checkin sensitive information &amp; credentials in version control. Instead <code>apikey</code> will be added to github secrets, so it can be called as environment variable.</p><h2 id="create-github-workflow-file">Create github workflow file</h2><pre><code class="language-yml">name: Deploy

on:
  push:
    branches: [ master, main ]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python 3.9
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml shub
    - name: Deploy to zyte
      if: github.ref == &apos;refs/heads/master&apos;
      run: python3 utils/edit_deploy_config.py &amp;&amp; shub deploy
      env:
        APIKEY: ${{ secrets.APIKEY }}</code></pre><p>Translation:</p><ul><li>On push to this repo (this doesn&apos;t work for PRs)</li><li>Download this repo</li><li>Setup python3.9</li><li>Install some pip modules</li><li>Run a script to overwrite <code>scrapinghub.yml</code>&apos;s <code>apikey</code> value, in which the value is obtained from github secrets</li><li>Execute deploy command</li></ul>]]></content:encoded></item><item><title><![CDATA[Elasticsearch with custom dictionary]]></title><description><![CDATA[<p>Elasticsearch is a search engine with built-in analyzers (combination of tokenizer and filters), which makes it easier to set it up and get it running, seeing you don&#x2019;t have to implement NLP logic from scratch. However, for some languages such as Thai, the built-in Thai analyzer may not</p>]]></description><link>https://blog.karnwong.me/elasticsearch-with-custom-dictionary/</link><guid isPermaLink="false">608fa00bc705a3000119bf19</guid><category><![CDATA[backend]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Mon, 03 May 2021 07:04:42 GMT</pubDate><content:encoded><![CDATA[<p>Elasticsearch is a search engine with built-in analyzers (combination of tokenizer and filters), which makes it easier to set it up and get it running, seeing you don&#x2019;t have to implement NLP logic from scratch. However, for some languages such as Thai, the built-in Thai analyzer may not be working quite as expected.</p><p>For instance, for region name search autocomplete, it <em>doesn&#x2019;t</em> recommend anything when I type <code>&#xE40;&#xE0A;&#xE35;&#xE22;&#xE07;</code>, but it should be showing <code>&#xE40;&#xE0A;&#xE35;&#xE22;&#xE07;&#xE43;&#xE2B;&#xE21;&#xE48;</code> or <code>&#xE40;&#xE0A;&#xE35;&#xE22;&#xE07;&#xE23;&#xE32;&#xE22;</code>. This is because these two regions are recognized as one token, which is why it doesn&#x2019;t recommend anything when querying with <code>&#xE40;&#xE0A;&#xE35;&#xE22;&#xE07;</code>.</p><p>But if I create a custom dictionary for tokenizers with <code>&#xE40;&#xE0A;&#xE35;&#xE22;&#xE07;</code> as one of the tokens, it manages to recommend the two regions when querying with the prefix.</p><p>Below is an <code>index_config</code> for using a custom dictionary for tokenizer:</p><pre><code class="language-python">{
    &quot;settings&quot;: {
        &quot;analysis&quot;: {
            &quot;analyzer&quot;: {
                &quot;thai_dictionary&quot;: {
                    &quot;tokenizer&quot;: &quot;standard&quot;,
                    &quot;filter&quot;: [
                        &quot;char_limit_dictionary&quot;
                    ]
                }
            },
            &quot;filter&quot;: {
                &quot;char_limit_dictionary&quot;: {
                    &quot;type&quot;: &quot;dictionary_decompounder&quot;,
                    &quot;word_list&quot;: tokens, # &lt;-- word list array here
                    &quot;max_subword_size&quot;: 22
                }
            }
        }
    },
    &quot;mappings&quot;: {
        &quot;properties&quot;: {
            &quot;title&quot;: { # &lt;-- search key
                &quot;type&quot;: &quot;text&quot;,
                &quot;analyzer&quot;: &quot;thai_dictionary&quot;
            }
        }
    }
}</code></pre><p>See elasticsearch documentation for more details: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.12/analysis-dict-decomp-tokenfilter.html" rel="noopener noreferrer nofollow">https://www.elastic.co/guide/en/elasticsearch/reference/7.12/analysis-dict-decomp-tokenfilter.html</a></p>]]></content:encoded></item><item><title><![CDATA[Shapefile to data lake]]></title><description><![CDATA[<p><strong>Background</strong>: we use spark to read/write to data lake. For dealing with spatial data &amp; analysis, we use <a href="http://sedona.apache.org">sedona</a>. Shapefile is converted to TSV then read by spark for further processing &amp; archival.</p><p>Recently I had to archive shapefiles in our data lake. It wasn&apos;t rosy for</p>]]></description><link>https://blog.karnwong.me/shapefile-to-data-lake/</link><guid isPermaLink="false">60830d38c705a3000119befd</guid><category><![CDATA[data engineering]]></category><category><![CDATA[gis]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Fri, 23 Apr 2021 18:25:13 GMT</pubDate><content:encoded><![CDATA[<p><strong>Background</strong>: we use spark to read/write to data lake. For dealing with spatial data &amp; analysis, we use <a href="http://sedona.apache.org">sedona</a>. Shapefile is converted to TSV then read by spark for further processing &amp; archival.</p><p>Recently I had to archive shapefiles in our data lake. It wasn&apos;t rosy for the following reasons:</p><h2 id="invalid-geometries">Invalid geometries</h2><p>Sedona (and geopandas too) whines if it encounters <code>invalid geometry</code> during geometry casting. The invalid geometries could be from many reasons, one of them being unclean polygon clipping.</p><p>Solution: use <code>gdal</code> to filter out invalid geometries.</p><h2 id="spatial-projection">Spatial projection</h2><p>Geometric projections requires <code>projection</code>, otherwise you could be on the wrong side of the globe. This matters because by default, the worldwide-coverage projection is <code>EPSG:4326</code>, but the unit is in <code>degrees</code>, so sometimes for analysis the data is converted to a local projection which covers a smaller geographical region, but uses <code>meter</code> as the unit.</p><p>This means that if the source projection is in <code>A</code>, and you didn&apos;t cast it to <code>EPSG:4326</code>, spark would mistakenly think it&apos;s on <code>EPSG:4326</code> by default. Something like seeing the entirely of the UK in Africa.</p><p>Solution: verify the source projection and cast to <code>EPSG:4326</code> before writing to data lake.</p><h2 id="extra-new-line-character">Extra new line character</h2><p>Sometimes when editing shapefile data by hand using applications like ArcGIS or QGIS, you could copy a text which might contain &quot;new line&quot; character, and set it as a cell value. Spark doesn&apos;t play nice with &quot;new line&quot; characters in a middle of a record.</p><p>Solution: strip new line characters by hand.<br><br>Yes, I really did that &#x1F636;. Thankfully it was a very small shapefile that has the issue.</p><p><strong>Takeaways</strong>: count yourself lucky if you never have to deal with spatial data.</p>]]></content:encoded></item><item><title><![CDATA[Spark join OOM fix]]></title><description><![CDATA[<p>I have a big pipelines where one step performs crossjoin on <code>130K x 7K</code>. It fails quite often, and I have to pray to the Rice God for it to pass. Today I found the solution: <code>repartition</code> before crossjoin.</p><p>The root cause is that the dataframe with 130K records has</p>]]></description><link>https://blog.karnwong.me/spark-join-oom-fix/</link><guid isPermaLink="false">60731f8b25ce9d0001e548a7</guid><category><![CDATA[big data]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sun, 11 Apr 2021 16:20:23 GMT</pubDate><content:encoded><![CDATA[<p>I have a big pipelines where one step performs crossjoin on <code>130K x 7K</code>. It fails quite often, and I have to pray to the Rice God for it to pass. Today I found the solution: <code>repartition</code> before crossjoin.</p><p>The root cause is that the dataframe with 130K records has 6 partitions, so when I perform crossjoin (one-to-many) it&apos;s working against those 6 partitions. Total output in parquet is around 350MB, which means my computer (8 cores, 10GB RAM provisioned for spark) needs to be able to hold all uncompressed data in memory. It couldn&apos;t hence the frequent OOM.</p><p>So by increasing the partition size from 6 to 24, the current working dataframe size is smaller, which means things could pass along faster while not filling up my machine&apos;s RAM.</p>]]></content:encoded></item><item><title><![CDATA[ถอดรหัสคำตอบเวลาถามเรื่องกลูเตน]]></title><description><![CDATA[<h2 id="%E0%B8%AD%E0%B9%88%E0%B8%B2%E0%B8%99%E0%B8%95%E0%B8%A3%E0%B8%87%E0%B8%99%E0%B8%B5%E0%B9%89-%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B8%A1">&#xE2D;&#xE48;&#xE32;&#xE19;&#xE15;&#xE23;&#xE07;&#xE19;&#xE35;&#xE49; &#xE2D;&#xE22;&#xE48;&#xE32;&#xE02;&#xE49;&#xE32;&#xE21;!!!!!</h2><p>&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE43;&#xE0A;&#xE48;&#xE1C;&#xE39;&#xE49;&#xE23;&#xE49;&#xE32;&#xE22; &#xE02;&#xE19;&#xE21;&#xE1B;&#xE31;&#xE07;&#xE17;&#xE35;&#xE48;&#xE40;&#xE14;&#xE49;&#xE07;</p>]]></description><link>https://blog.karnwong.me/th-drhaskhamt-bewlaathaameruue-ngkluuetn/</link><guid isPermaLink="false">6064717f25ce9d0001e54800</guid><category><![CDATA[celiac]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Wed, 31 Mar 2021 13:35:55 GMT</pubDate><content:encoded><![CDATA[<h2 id="%E0%B8%AD%E0%B9%88%E0%B8%B2%E0%B8%99%E0%B8%95%E0%B8%A3%E0%B8%87%E0%B8%99%E0%B8%B5%E0%B9%89-%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B8%A1">&#xE2D;&#xE48;&#xE32;&#xE19;&#xE15;&#xE23;&#xE07;&#xE19;&#xE35;&#xE49; &#xE2D;&#xE22;&#xE48;&#xE32;&#xE02;&#xE49;&#xE32;&#xE21;!!!!!</h2><p>&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE43;&#xE0A;&#xE48;&#xE1C;&#xE39;&#xE49;&#xE23;&#xE49;&#xE32;&#xE22; &#xE02;&#xE19;&#xE21;&#xE1B;&#xE31;&#xE07;&#xE17;&#xE35;&#xE48;&#xE40;&#xE14;&#xE49;&#xE07;&#xE14;&#xE36;&#xE4B;&#xE07;&#xE19;&#xE35;&#xE48;&#xE01;&#xE47;&#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE1D;&#xE35;&#xE21;&#xE37;&#xE2D;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; &#xE2D;&#xE30;&#xE44;&#xE23;&#xE17;&#xE35;&#xE48;&#xE17;&#xE33;&#xE21;&#xE32;&#xE08;&#xE32;&#xE01;&#xE41;&#xE1B;&#xE49;&#xE07; &#xE01;&#xE34;&#xE19;&#xE41;&#xE25;&#xE49;&#xE27;&#xE21;&#xE35;&#xE04;&#xE27;&#xE32;&#xE21;&#xE2B;&#xE19;&#xE36;&#xE1A;&#xE2B;&#xE19;&#xE31;&#xE1A; &#xE1F;&#xE31;&#xE19;&#xE18;&#xE07;&#xE44;&#xE14;&#xE49;&#xE40;&#xE25;&#xE22;&#xE27;&#xE48;&#xE32;&#xE1D;&#xE35;&#xE21;&#xE37;&#xE2D;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; &#xE41;&#xE1B;&#xE49;&#xE07;&#xE2A;&#xE32;&#xE25;&#xE35;&#xE01;&#xE47;&#xE21;&#xE35;&#xE2A;&#xE32;&#xE23;&#xE2D;&#xE32;&#xE2B;&#xE32;&#xE23;&#xE40;&#xE22;&#xE2D;&#xE30;&#xE01;&#xE27;&#xE48;&#xE32;&#xE41;&#xE1B;&#xE49;&#xE07;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE1F;&#xE23;&#xE35; &#xE41;&#xE15;&#xE48;&#xE21;&#xE19;&#xE38;&#xE14;&#xE1A;&#xE32;&#xE07;&#xE08;&#xE33;&#xE1E;&#xE27;&#xE01;&#xE01;&#xE34;&#xE19;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49; &#xE41;&#xE25;&#xE49;&#xE27;&#xE1A;&#xE32;&#xE07;&#xE04;&#xE19;&#xE2B;&#xE31;&#xE27;&#xE43;&#xE2A;&#xE40;&#xE2D;&#xE32;&#xE44;&#xE1B;&#xE42;&#xE1B;&#xE23;&#xE42;&#xE21;&#xE17;&#xE27;&#xE48;&#xE32; &#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE1F;&#xE23;&#xE35;&#xE21;&#xE31;&#xE19;&#xE40;&#xE2E;&#xE25;&#xE15;&#xE35;&#xE49; &#xE04;&#xE19;&#xE40;&#xE25;&#xE22;&#xE41;&#xE2B;&#xE48;&#xE01;&#xE31;&#xE19;&#xE44;&#xE1B;&#xE17;&#xE33;&#xE2D;&#xE32;&#xE2B;&#xE32;&#xE23;&#xE01;&#xE31;&#xE1A;&#xE02;&#xE19;&#xE21;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE1F;&#xE23;&#xE35;&#xE2D;&#xE2D;&#xE01;&#xE21;&#xE32; &#xE41;&#xE15;&#xE48;&#xE40;&#xE01;&#xE37;&#xE2D;&#xE1A;&#xE2B;&#xE21;&#xE14;&#xE19;&#xE31;&#xE49;&#xE19;<strong>&#xE04;&#xE19;&#xE17;&#xE35;&#xE48;&#xE41;&#xE1E;&#xE49;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE08;&#xE23;&#xE34;&#xE07;&#xE46; &#xE01;&#xE34;&#xE19;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49;!</strong></p><h2 id="%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%87">&#xE40;&#xE02;&#xE49;&#xE32;&#xE40;&#xE23;&#xE37;&#xE48;&#xE2D;&#xE07;</h2><p>&#xE2A;&#xE48;&#xE27;&#xE19;&#xE19;&#xE35;&#xE49;&#xE08;&#xE30;&#xE40;&#xE1B;&#xE47;&#xE19;&#xE04;&#xE33;&#xE15;&#xE2D;&#xE1A;&#xE17;&#xE35;&#xE48;&#xE15;&#xE31;&#xE27;&#xE41;&#xE17;&#xE19;&#xE08;&#xE32;&#xE01;&#xE1C;&#xE39;&#xE49;&#xE1C;&#xE25;&#xE34;&#xE15; (&#xE04;&#xE19;&#xE17;&#xE35;&#xE48;&#xE23;&#xE31;&#xE1A;&#xE2A;&#xE32;&#xE22;&#xE19;&#xE48;&#xE30;&#xE41;&#xE2B;&#xE25;&#xE30;) &#xE15;&#xE2D;&#xE1A; &#xE0B;&#xE36;&#xE48;&#xE07;&#xE08;&#xE30;&#xE2D;&#xE18;&#xE34;&#xE1A;&#xE32;&#xE22;&#xE2B;&#xE21;&#xE32;&#xE22;&#xE40;&#xE2B;&#xE15;&#xE38;++ &#xE27;&#xE48;&#xE32;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE41;&#xE1B;&#xE25;&#xE41;&#xE15;&#xE48;&#xE25;&#xE30;&#xE04;&#xE33;&#xE15;&#xE2D;&#xE1A;&#xE22;&#xE31;&#xE07;&#xE44;&#xE07;</p><hr><!--kg-card-begin: markdown--><blockquote>
<p>&#xE40;&#xE14;&#xE35;&#xE4B;&#xE22;&#xE27;&#xE15;&#xE34;&#xE14;&#xE15;&#xE48;&#xE2D;&#xE01;&#xE25;&#xE31;&#xE1A;&#xE19;&#xE30;&#xE04;&#xE23;&#xE31;&#xE1A; &#xE02;&#xE2D;&#xE44;&#xE1B;&#xE2B;&#xE32;&#xE02;&#xE49;&#xE2D;&#xE21;&#xE39;&#xE25;&#xE01;&#xE48;&#xE2D;&#xE19;</p>
</blockquote>
<p>&#xE40;&#xE01;&#xE37;&#xE2D;&#xE1A;&#xE17;&#xE31;&#xE49;&#xE07;&#xE2B;&#xE21;&#xE14;&#xE17;&#xE35;&#xE48;&#xE15;&#xE2D;&#xE1A;&#xE41;&#xE1A;&#xE1A;&#xE19;&#xE35;&#xE49; &#xE08;&#xE30;&#xE44;&#xE21;&#xE48;&#xE15;&#xE34;&#xE14;&#xE15;&#xE48;&#xE2D;&#xE01;&#xE25;&#xE31;&#xE1A;&#xE21;&#xE32;&#xE2D;&#xE35;&#xE01;&#xE40;&#xE25;&#xE22; &#x2601;&#xFE0F;&#x2601;&#xFE0F;&#x2601;&#xFE0F;</p>
<blockquote>
<p>&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE02;&#xE2D;&#xE07;&#xE40;&#xE23;&#xE32;&#xE44;&#xE21;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE40;&#xE25;&#xE22;&#xE04;&#xE48;&#xE30; &#xE27;&#xE31;&#xE15;&#xE16;&#xE38;&#xE14;&#xE34;&#xE1A;&#xE01;&#xE47;&#xE21;&#xE35;&#xE41;&#xE04;&#xE48; A, B C &#xE0B;&#xE36;&#xE48;&#xE07;&#xE17;&#xE31;&#xE49;&#xE07;&#xE2B;&#xE21;&#xE14;&#xE19;&#xE35;&#xE49;&#xE44;&#xE21;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE2D;&#xE22;&#xE39;&#xE48;&#xE41;&#xE25;&#xE49;&#xE27;&#xE15;&#xE32;&#xE21;&#xE18;&#xE23;&#xE23;&#xE21;&#xE0A;&#xE32;&#xE15;&#xE34;</p>
</blockquote>
<p>&#xE2D;&#xE31;&#xE19;&#xE27;&#xE48;&#xE32; cross-contamination &#xE19;&#xE31;&#xE49;&#xE19;&#xE01;&#xE47;&#xE40;&#xE2B;&#xE21;&#xE37;&#xE2D;&#xE19; &#x1F4A9; &#xE01;&#xE47;&#xE04;&#xE37;&#xE2D; &#xE2D;&#xE30;&#xE44;&#xE23;&#xE17;&#xE35;&#xE48;&#xE21;&#xE31;&#xE19;&#xE44;&#xE1B;&#xE42;&#xE14;&#xE19; &#xE01;&#xE47;&#xE08;&#xE30; &#x1F92E; &#xE15;&#xE32;&#xE21;&#xE44;&#xE1B;&#xE14;&#xE49;&#xE27;&#xE22; &#xE15;&#xE48;&#xE2D;&#xE43;&#xE2B;&#xE49;&#xE08;&#xE23;&#xE34;&#xE07;&#xE46; &#xE41;&#xE25;&#xE49;&#xE27;&#xE21;&#xE31;&#xE19;&#xE2A;&#xE30;&#xE2D;&#xE32;&#xE14;&#xE01;&#xE47;&#xE15;&#xE32;&#xE21;</p>
<p>&#xE0B;&#xE36;&#xE48;&#xE07;&#xE01;&#xE47;&#xE41;&#xE1B;&#xE25;&#xE27;&#xE48;&#xE32; &#xE15;&#xE48;&#xE2D;&#xE43;&#xE2B;&#xE49;&#xE27;&#xE31;&#xE15;&#xE16;&#xE38;&#xE14;&#xE34;&#xE1A;<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> &#xE08;&#xE30;&#xE1B;&#xE25;&#xE2D;&#xE14;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE41;&#xE04;&#xE48;&#xE44;&#xE2B;&#xE19;&#xE01;&#xE47;&#xE15;&#xE32;&#xE21; &#xE41;&#xE15;&#xE48;&#xE16;&#xE49;&#xE32;&#xE21;&#xE31;&#xE19;&#xE17;&#xE33;&#xE1A;&#xE19;&#xE44;&#xE25;&#xE19;&#xE4C;&#xE1C;&#xE25;&#xE34;&#xE15;&#xE40;&#xE14;&#xE35;&#xE22;&#xE27;&#xE01;&#xE31;&#xE1A;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE2D;&#xE37;&#xE48;&#xE19;&#xE17;&#xE35;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; &#xE01;&#xE47;&#xE44;&#xE21;&#xE48;&#xE23;&#xE2D;&#xE14;&#xE2D;&#xE22;&#xE39;&#xE48;&#xE14;&#xE35;<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>&#xE2A;&#xE33;&#xE2B;&#xE23;&#xE31;&#xE1A;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE1E;&#xE27;&#xE01; homemade &#xE17;&#xE31;&#xE49;&#xE07;&#xE2B;&#xE25;&#xE32;&#xE22;&#xE41;&#xE2B;&#xE25;&#xE48; &#xE15;&#xE48;&#xE2D;&#xE43;&#xE2B;&#xE49;&#xE08;&#xE30; gluten free &#xE40;&#xE1A;&#xE2D;&#xE23;&#xE4C;&#xE44;&#xE2B;&#xE19; &#xE16;&#xE49;&#xE32;&#xE40;&#xE02;&#xE32;&#xE1A;&#xE2D;&#xE01;&#xE27;&#xE48;&#xE32;&#xE43;&#xE0A;&#xE49;&#xE40;&#xE15;&#xE32;&#xE2D;&#xE1A;&#xE2D;&#xE31;&#xE19;&#xE40;&#xE14;&#xE35;&#xE22;&#xE27;&#xE01;&#xE31;&#xE19;&#xE01;&#xE31;&#xE1A;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE1B;&#xE01;&#xE15;&#xE34;(&#xE17;&#xE35;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;) &#xE01;&#xE47;&#xE08;&#xE07;&#xE43;&#xE2A;&#xE48;&#xE40;&#xE01;&#xE35;&#xE22;&#xE23;&#xE4C;&#xE2B;&#xE21;&#xE32;&#xE0B;&#xE30; &#xE40;&#xE2B;&#xE15;&#xE38;&#xE1C;&#xE25;&#xE40;&#xE14;&#xE35;&#xE22;&#xE27;&#xE01;&#xE31;&#xE1A;&#xE02;&#xE49;&#xE32;&#xE07;&#xE1A;&#xE19;</p>
<blockquote>
<p>&#xE17;&#xE32;&#xE07;&#xE40;&#xE23;&#xE32;&#xE44;&#xE21;&#xE48;&#xE41;&#xE19;&#xE30;&#xE19;&#xE33;&#xE43;&#xE2B;&#xE49;&#xE25;&#xE39;&#xE01;&#xE04;&#xE49;&#xE32;&#xE17;&#xE32;&#xE19;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE40;&#xE23;&#xE32;&#xE04;&#xE48;&#xE30; &#xE40;&#xE1E;&#xE37;&#xE48;&#xE2D;&#xE04;&#xE27;&#xE32;&#xE21;&#xE1B;&#xE25;&#xE2D;&#xE14;&#xE20;&#xE31;&#xE22;</p>
</blockquote>
<ol>
<li>&#xE1E;&#xE35;&#xE04;&#xE15;&#xE23;&#xE07;&#xE17;&#xE35;&#xE48;&#xE09;&#xE25;&#xE32;&#xE01;&#xE44;&#xE21;&#xE48;&#xE40;&#xE02;&#xE35;&#xE22;&#xE19;&#xE27;&#xE48;&#xE32;(&#xE2D;&#xE32;&#xE08;&#xE08;&#xE30;&#xE21;&#xE35;&#xE2A;&#xE48;&#xE27;&#xE19;&#xE1C;&#xE2A;&#xE21;&#xE02;&#xE2D;&#xE07;)&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; -&gt; &#xE01;&#xE47;&#xE40;&#xE02;&#xE35;&#xE22;&#xE19;&#xE41;&#xE1B;&#xE30;&#xE2A;&#xE34;&#xE27;&#xE48;&#xE32;&#xE2D;&#xE32;&#xE08;&#xE08;&#xE30;&#xE21;&#xE35;&#xE2A;&#xE48;&#xE27;&#xE19;&#xE1C;&#xE2A;&#xE21;&#xE02;&#xE2D;&#xE07;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; &#xE44;&#xE21;&#xE48;&#xE43;&#xE0A;&#xE48;&#xE43;&#xE2B;&#xE49;&#xE04;&#xE19;&#xE01;&#xE34;&#xE19;&#xE21;&#xE32;&#xE25;&#xE38;&#xE49;&#xE19;&#xE22;&#xE31;&#xE07;&#xE01;&#xE30;&#xE08;&#xE31;&#xE1A;&#xE43;&#xE1A;&#xE14;&#xE33;&#xE43;&#xE1A;&#xE41;&#xE14;&#xE07;</li>
<li>&#xE04;&#xE19;&#xE15;&#xE2D;&#xE1A;&#xE40;&#xE2D;&#xE07;&#xE19;&#xE31;&#xE48;&#xE19;&#xE41;&#xE2B;&#xE25;&#xE30;&#xE17;&#xE35;&#xE48;&#xE44;&#xE21;&#xE48;&#xE01;&#xE25;&#xE49;&#xE32;&#xE23;&#xE31;&#xE1A;&#xE1B;&#xE32;&#xE01;&#xE2D;&#xE30;&#xE44;&#xE23;&#xE17;&#xE31;&#xE49;&#xE07;&#xE19;&#xE31;&#xE49;&#xE19; &#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE44;&#xE21;&#xE48;&#xE23;&#xE39;&#xE49;&#xE02;&#xE49;&#xE2D;&#xE21;&#xE39;&#xE25; &#xE40;&#xE25;&#xE22;&#xE1E;&#xE22;&#xE32;&#xE22;&#xE32;&#xE21;&#xE1B;&#xE31;&#xE14;&#xE43;&#xE2B;&#xE49;&#xE1E;&#xE49;&#xE19;&#xE15;&#xE31;&#xE27;</li>
</ol>
<blockquote>
<p>&#xE09;&#xE25;&#xE32;&#xE01;&#xE02;&#xE2D;&#xE07;&#xE40;&#xE23;&#xE32;&#xE1C;&#xE48;&#xE32;&#xE19;&#xE2D;.&#xE22;. &#xE04;&#xE48;&#xE30; &#xE2D;&#xE34;&#xE07;&#xE15;&#xE32;&#xE21;&#xE01;&#xE0E;&#xE2B;&#xE21;&#xE32;&#xE22;&#xE44;&#xE17;&#xE22;</p>
</blockquote>
<p>&#xE16;&#xE49;&#xE32;&#xE44;&#xE14;&#xE49;&#xE04;&#xE33;&#xE15;&#xE2D;&#xE1A;&#xE41;&#xE1A;&#xE1A;&#xE19;&#xE35;&#xE49;&#xE21;&#xE32; &#xE08;&#xE07;&#xE27;&#xE34;&#xE48;&#xE07;&#xE2B;&#xE19;&#xE35;&#xE2A;&#xE38;&#xE14;&#xE0A;&#xE35;&#xE27;&#xE34;&#xE15; &#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE2D;.&#xE22;.&#xE44;&#xE17;&#xE22;&#xE44;&#xE21;&#xE48;&#xE41;&#xE04;&#xE23;&#xE4C;&#xE43;&#xE14;&#xE46; &#xE17;&#xE31;&#xE49;&#xE07;&#xE19;&#xE31;&#xE49;&#xE19; &#xE15;&#xE32;&#xE21;&#xE17;&#xE49;&#xE2D;&#xE07;&#xE15;&#xE25;&#xE32;&#xE14;&#xE21;&#xE35;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE17;&#xE35;&#xE48;&#xE21;&#xE35;&#xE0B;&#xE35;&#xE2D;&#xE34;&#xE4A;&#xE27; &#xE41;&#xE15;&#xE48;&#xE41;&#xE08;&#xE49;&#xE07;&#xE41;&#xE04;&#xE48; &#xE21;&#xE35;&#xE2A;&#xE48;&#xE27;&#xE19;&#xE1C;&#xE2A;&#xE21;&#xE02;&#xE2D;&#xE07;&#xE16;&#xE31;&#xE48;&#xE27;&#xE40;&#xE2B;&#xE25;&#xE37;&#xE2D;&#xE07; &#xE40;&#xE22;&#xE2D;&#xE30;&#xE21;&#xE32;&#xE01;&#xE01;&#xE01;&#xE01;&#xE01;&#xE01;&#xE01; (&#xE0B;&#xE35;&#xE2D;&#xE34;&#xE4A;&#xE27;&#xE2B;&#xE21;&#xE31;&#xE01;&#xE01;&#xE31;&#xE1A;&#xE41;&#xE1B;&#xE49;&#xE07;&#xE2A;&#xE32;&#xE25;&#xE35;) &#xE41;&#xE25;&#xE30; &#xE2D;.&#xE22;.&#xE23;&#xE30;&#xE1A;&#xE38;&#xE44;&#xE27;&#xE49;&#xE27;&#xE48;&#xE32; &#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE17;&#xE35;&#xE48;&#xE2A;&#xE48;&#xE27;&#xE19;&#xE1C;&#xE2A;&#xE21;&#xE2B;&#xE25;&#xE31;&#xE01;&#xE44;&#xE21;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19; &#xE44;&#xE21;&#xE48;&#xE2A;&#xE32;&#xE21;&#xE32;&#xE23;&#xE16;&#xE40;&#xE02;&#xE35;&#xE22;&#xE19;&#xE1A;&#xE19;&#xE09;&#xE25;&#xE32;&#xE01;&#xE27;&#xE48;&#xE32; gluten free<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p>
<blockquote>
<p>&#xE09;&#xE25;&#xE32;&#xE01;&#xE02;&#xE2D;&#xE07;&#xE40;&#xE23;&#xE32;&#xE16;&#xE39;&#xE01;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE15;&#xE32;&#xE21;&#xE01;&#xE0E;&#xE2B;&#xE21;&#xE32;&#xE22;&#xE04;&#xE48;&#xE30; &#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE21;&#xE31;&#xE19;&#xE21;&#xE35; customer protection law &#xE2D;&#xE22;&#xE39;&#xE48; &#xE44;&#xE21;&#xE48;&#xE07;&#xE31;&#xE49;&#xE19;&#xE01;&#xE47;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE08;&#xE48;&#xE32;&#xE22;&#xE04;&#xE48;&#xE32;&#xE40;&#xE2A;&#xE35;&#xE22;&#xE2B;&#xE32;&#xE22;</p>
</blockquote>
<p>&#xE08;&#xE07;&#xE2B;&#xE19;&#xE35;&#xE43;&#xE2B;&#xE49;&#xE44;&#xE27; &#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE17;&#xE35;&#xE48;&#xE40;&#xE21;&#xE01;&#xE32;&#xE21;&#xE35;&#xE40;&#xE04;&#xE2A;&#xE04;&#xE19;&#xE01;&#xE34;&#xE19; Cheerios<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> (&#xE04;&#xE19;&#xE17;&#xE35;&#xE48;&#xE01;&#xE34;&#xE19;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49;&#xE23;&#xE39;&#xE49;&#xE01;&#xE31;&#xE19;&#xE27;&#xE48;&#xE32; gluten free &#xE22;&#xE35;&#xE48;&#xE2B;&#xE49;&#xE2D;&#xE19;&#xE35;&#xE49;&#xE21;&#xE31;&#xE19;&#xE40;&#xE01;&#xE4A;&#xE22;&#xE34;&#xE48;&#xE07;&#xE01;&#xE27;&#xE48;&#xE32;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE40;&#xE2A;&#xE34;&#xE48;&#xE19;&#xE40;&#xE08;&#xE34;&#xE49;&#xE19;) &#xE41;&#xE25;&#xE49;&#xE27;&#xE2D;&#xE32;&#xE01;&#xE32;&#xE23;&#xE2B;&#xE19;&#xE31;&#xE01;&#xE08;&#xE19;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE40;&#xE02;&#xE49;&#xE32;&#xE2B;&#xE49;&#xE2D;&#xE07;&#xE09;&#xE38;&#xE01;&#xE40;&#xE09;&#xE34;&#xE19; &#xE2A;&#xE34;&#xE48;&#xE07;&#xE17;&#xE35;&#xE48;&#xE41;&#xE1A;&#xE23;&#xE19;&#xE14;&#xE4C;&#xE19;&#xE35;&#xE49;&#xE17;&#xE33; &#xE04;&#xE37;&#xE2D; &#xE08;&#xE48;&#xE32;&#xE22;&#xE04;&#xE48;&#xE32;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE0A;&#xE14;&#xE40;&#xE0A;&#xE22;&#xE43;&#xE2B;&#xE49; &#xE40;&#xE09;&#xE1E;&#xE32;&#xE30;&#xE04;&#xE48;&#xE32;&#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32; &#xE04;&#xE48;&#xE32;&#xE2B;&#xE49;&#xE2D;&#xE07;&#xE09;&#xE38;&#xE01;&#xE40;&#xE09;&#xE34;&#xE19;&#xE44;&#xE1B;&#xE08;&#xE48;&#xE32;&#xE22;&#xE01;&#xE31;&#xE19;&#xE40;&#xE2D;&#xE32;&#xE40;&#xE2D;&#xE07;</p>
<blockquote>
<p>&#xE41;&#xE15;&#xE48;&#xE25;&#xE30;&#xE04;&#xE19;&#xE40;&#xE0B;&#xE19;&#xE0B;&#xE34;&#xE17;&#xE35;&#xE1F;&#xE44;&#xE21;&#xE48;&#xE40;&#xE2B;&#xE21;&#xE37;&#xE2D;&#xE19;&#xE04;&#xE48;&#xE30;&#xE04;&#xE38;&#xE13;&#xE25;&#xE39;&#xE01;&#xE04;&#xE49;&#xE32;</p>
</blockquote>
<p>&#xE40;&#xE2D;&#xE49;&#xE32; &#xE1A;&#xE17;&#xE04;&#xE19;&#xE01;&#xE34;&#xE19;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49;&#xE01;&#xE47;&#xE04;&#xE37;&#xE2D;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49; &#xE1A;&#xE32;&#xE07;&#xE04;&#xE19;&#xE01;&#xE34;&#xE19;&#xE44;&#xE14;&#xE49;&#xE21;&#xE32;&#xE01;&#xE19;&#xE49;&#xE2D;&#xE22;<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> &#xE41;&#xE15;&#xE48;&#xE16;&#xE49;&#xE32;&#xE17;&#xE33;&#xE41;&#xE1A;&#xE1A;&#xE04;&#xE19;&#xE40;&#xE0B;&#xE19;&#xE0B;&#xE34;&#xE17;&#xE35;&#xE1F;&#xE21;&#xE32;&#xE01;&#xE46; &#xE01;&#xE34;&#xE19;&#xE44;&#xE14;&#xE49;&#xE01;&#xE47;&#xE08;&#xE1A;&#xE2B;&#xE19;&#xE34;<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> &#xE1B;&#xE48;&#xE30;?</p>
<blockquote>
<p>(&#xE15;&#xE2D;&#xE1A;&#xE44;&#xE21;&#xE48;&#xE15;&#xE23;&#xE07;&#xE04;&#xE33;&#xE16;&#xE32;&#xE21; &#xE1E;&#xE39;&#xE14;&#xE27;&#xE19;&#xE44;&#xE1B;&#xE27;&#xE19;&#xE21;&#xE32;)</p>
</blockquote>
<p>&#xE27;&#xE32;&#xE07;&#xE2A;&#xE32;&#xE22;&#xE42;&#xE14;&#xE22;&#xE1E;&#xE25;&#xE31;&#xE19; &#xE40;&#xE1E;&#xE23;&#xE32;&#xE30;&#xE40;&#xE02;&#xE32;&#xE40;&#xE25;&#xE35;&#xE48;&#xE22;&#xE07;&#xE44;&#xE21;&#xE48;&#xE23;&#xE31;&#xE1A;&#xE1B;&#xE32;&#xE01;&#xE2D;&#xE30;&#xE44;&#xE23;&#xE17;&#xE31;&#xE49;&#xE07;&#xE19;&#xE31;&#xE49;&#xE19; &#xE01;&#xE25;&#xE31;&#xE27;&#xE25;&#xE39;&#xE01;&#xE04;&#xE49;&#xE32;&#xE2D;&#xE31;&#xE14;&#xE40;&#xE2A;&#xE35;&#xE22;&#xE07;&#xE44;&#xE27;&#xE49;&#xE41;&#xE25;&#xE49;&#xE27;&#xE40;&#xE2D;&#xE32;&#xE44;&#xE1B;&#xE43;&#xE0A;&#xE49;&#xE40;&#xE1B;&#xE47;&#xE19;&#xE2B;&#xE25;&#xE31;&#xE01;&#xE10;&#xE32;&#xE19;&#xE43;&#xE19;&#xE28;&#xE32;&#xE25; &#xE40;&#xE01;&#xE34;&#xE14;&#xE0B;&#xE37;&#xE49;&#xE2D;&#xE44;&#xE1B;&#xE01;&#xE34;&#xE19;&#xE41;&#xE25;&#xE49;&#xE27;&#xE2D;&#xE32;&#xE01;&#xE32;&#xE23;&#xE2D;&#xE2D;&#xE01;&#xE17;&#xE31;&#xE49;&#xE07;&#xE46; &#xE17;&#xE35;&#xE48;&#xE23;&#xE31;&#xE1A;&#xE1B;&#xE32;&#xE01;&#xE41;&#xE25;&#xE49;&#xE27;<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>&#xE41;&#xE15;&#xE48;&#xE16;&#xE49;&#xE32;&#xE17;&#xE31;&#xE49;&#xE07;&#xE42;&#xE23;&#xE07;&#xE07;&#xE32;&#xE19;&#xE44;&#xE21;&#xE48;&#xE21;&#xE35;&#xE01;&#xE32;&#xE23;&#xE43;&#xE0A;&#xE49;&#xE27;&#xE31;&#xE15;&#xE16;&#xE38;&#xE14;&#xE34;&#xE1A;&#xE17;&#xE35;&#xE48;&#xE21;&#xE35;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE40;&#xE25;&#xE22; &#xE01;&#xE47;&#xE22;&#xE31;&#xE07;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE15;&#xE32;&#xE21;&#xE44;&#xE1B;&#xE40;&#xE0A;&#xE47;&#xE04;&#xE27;&#xE48;&#xE32; &#xE27;&#xE31;&#xE15;&#xE16;&#xE38;&#xE14;&#xE34;&#xE1A;&#xE17;&#xE35;&#xE48;&#xE40;&#xE02;&#xE32;&#xE23;&#xE31;&#xE1A;&#xE21;&#xE32;&#xE21;&#xE35;&#xE01;&#xE32;&#xE23;&#xE1B;&#xE19;&#xE40;&#xE1B;&#xE37;&#xE49;&#xE2D;&#xE19;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE08;&#xE32;&#xE01;&#xE01;&#xE32;&#xE23;&#xE1A;&#xE27;&#xE19;&#xE01;&#xE32;&#xE23;&#xE02;&#xE19;&#xE2A;&#xE48;&#xE07;&#xE15;&#xE31;&#xE49;&#xE07;&#xE41;&#xE15;&#xE48;&#xE17;&#xE32;&#xE07; supplier &#xE21;&#xE31;&#xE49;&#xE22; <a href="#fnref1" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn2" class="footnote-item"><p>&#xE2D;&#xE32;&#xE08;&#xE08;&#xE30;&#xE23;&#xE2D;&#xE14; &#xE16;&#xE49;&#xE32;&#xE21;&#xE35;&#xE01;&#xE32;&#xE23;&#xE17;&#xE33;&#xE04;&#xE27;&#xE32;&#xE21;&#xE2A;&#xE30;&#xE2D;&#xE32;&#xE14;&#xE44;&#xE25;&#xE19;&#xE4C;&#xE1C;&#xE25;&#xE34;&#xE15; &#xE41;&#xE15;&#xE48;&#xE01;&#xE47;&#xE15;&#xE49;&#xE2D;&#xE07;&#xE16;&#xE32;&#xE21;&#xE2B;&#xE32;&#xE1C;&#xE25;&#xE15;&#xE23;&#xE27;&#xE08;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE15;&#xE01;&#xE04;&#xE49;&#xE32;&#xE07;&#xE14;&#xE49;&#xE27;&#xE22;&#xE27;&#xE48;&#xE32;&#xE15;&#xE48;&#xE33;&#xE01;&#xE27;&#xE48;&#xE32; 20 PPM (mg/L -&gt; 1 &#xE2A;&#xE48;&#xE27;&#xE19;&#xE43;&#xE19;&#xE25;&#xE49;&#xE32;&#xE19;) &#xE21;&#xE31;&#xE49;&#xE22; <a href="#fnref2" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn3" class="footnote-item"><p>&#xE19;&#xE21;&#xE23;&#xE2A;&#xE21;&#xE2D;&#xE25;&#xE15;&#xE4C; &#xE42;&#xE01;&#xE42;&#xE01;&#xE49;&#xE1C;&#xE2A;&#xE21;&#xE21;&#xE2D;&#xE25;&#xE15;&#xE4C; &#xE41;&#xE25;&#xE30;&#xE2D;&#xE37;&#xE48;&#xE19;&#xE46; &#xE2D;&#xE35;&#xE01;&#xE21;&#xE32;&#xE01;&#xE21;&#xE32;&#xE22; <a href="#fnref3" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://topclassactions.com/lawsuit-settlements/consumer-products/329574-general-mills-faces-new-class-action-over-gluten-free-cheerios/">https://topclassactions.com/lawsuit-settlements/consumer-products/329574-general-mills-faces-new-class-action-over-gluten-free-cheerios/</a> <a href="#fnref4" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn5" class="footnote-item"><p>&#xE21;&#xE19;&#xE38;&#xE14;&#xE17;&#xE35;&#xE48;&#xE01;&#xE34;&#xE19;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49;&#xE21;&#xE35;&#xE2A;&#xE32;&#xE21;&#xE08;&#xE33;&#xE1E;&#xE27;&#xE01;: celiac disease, gluten intolerance, wheat allergy &#xE21;&#xE19;&#xE38;&#xE14; celiac disease &#xE08;&#xE30;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE19;&#xE49;&#xE2D;&#xE22;&#xE41;&#xE04;&#xE48;&#xE44;&#xE2B;&#xE19;&#xE01;&#xE47;&#xE44;&#xE21;&#xE48;&#xE44;&#xE14;&#xE49; (&#xE2D;&#xE48;&#xE30; &#xE22;&#xE2D;&#xE21;&#xE44;&#xE14;&#xE49;&#xE41;&#xE04;&#xE48; 20 PPM &#xE15;&#xE48;&#xE2D;&#xE27;&#xE31;&#xE19;) &#xE2A;&#xE48;&#xE27;&#xE19;&#xE2D;&#xE35;&#xE01;&#xE2A;&#xE2D;&#xE07;&#xE1E;&#xE27;&#xE01;&#xE17;&#xE35;&#xE48;&#xE40;&#xE2B;&#xE25;&#xE37;&#xE2D;&#xE2D;&#xE32;&#xE08;&#xE08;&#xE30;&#xE17;&#xE19;&#xE01;&#xE34;&#xE19;&#xE44;&#xE14;&#xE49;&#xE1A;&#xE49;&#xE32;&#xE07;&#xE40;&#xE25;&#xE47;&#xE01;&#xE19;&#xE49;&#xE2D;&#xE22; <a href="#fnref5" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn6" class="footnote-item"><p>EU &#xE01;&#xE33;&#xE2B;&#xE19;&#xE14;&#xE44;&#xE27;&#xE49;&#xE27;&#xE48;&#xE32; &#xE2A;&#xE34;&#xE19;&#xE04;&#xE49;&#xE32;&#xE43;&#xE14;&#xE46; &#xE17;&#xE35;&#xE48;&#xE1B;&#xE23;&#xE34;&#xE21;&#xE32;&#xE13;&#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE2A;&#xE39;&#xE07;&#xE01;&#xE27;&#xE48;&#xE32; 20 PPM <strong>&#xE44;&#xE21;&#xE48;&#xE2A;&#xE32;&#xE21;&#xE32;&#xE23;&#xE16;&#xE41;&#xE1B;&#xE30;&#xE44;&#xE14;&#xE49;&#xE27;&#xE48;&#xE32; &#xE01;&#xE25;&#xE39;&#xE40;&#xE15;&#xE19;&#xE1F;&#xE23;&#xE35;</strong> <a href="#fnref6" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn7" class="footnote-item"><p>&#xE40;&#xE02;&#xE32;&#xE15;&#xE2D;&#xE1A;&#xE40;&#xE23;&#xE32;&#xE07;&#xE35;&#xE49;&#xE08;&#xE23;&#xE34;&#xE07;&#xE46; &#xE19;&#xE30; <a href="#fnref7" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Add Ghost content to Hugo]]></title><description><![CDATA[<p>Ghost CMS is very easy to use, but the deployment overhead (maintaining db, ghost version, updates and etc) might be too much for some. Luckily, there&apos;s a way to convert a Ghost site to static pages, which you can later host on Github pages or something similar.</p><h2 id="setup">Setup:</h2>]]></description><link>https://blog.karnwong.me/create-static-site-from-ghost-blog/</link><guid isPermaLink="false">6063f27625ce9d0001e547be</guid><category><![CDATA[infra]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Wed, 31 Mar 2021 04:05:02 GMT</pubDate><content:encoded><![CDATA[<p>Ghost CMS is very easy to use, but the deployment overhead (maintaining db, ghost version, updates and etc) might be too much for some. Luckily, there&apos;s a way to convert a Ghost site to static pages, which you can later host on Github pages or something similar.</p><h2 id="setup">Setup:</h2><ul><li>static site engine: Hugo</li><li>a Ghost instance</li></ul><h2 id="usage">Usage</h2><ol><li>Install <a href="https://github.com/Fried-Chicken/ghost-static-site-generator">https://github.com/Fried-Chicken/ghost-static-site-generator</a></li><li><code>cd</code> to <code>static</code> directory in your Hugo folder</li><li>run <code>gssg --domain $<a href="https://blog.karnwong.me/">{</a>YOUR_GHOST_INSTANCE_URL} --dest posts --url $<a href="https://blog.karnwong.me/">{</a>YOUR_STATIC_SITE_DOMAIN_WITHOUT_TRAILING_SLASH} --subDir posts</code></li><li>Update your hugo config to link to the above folder:</li></ol><pre><code class="language-toml">[[menu.main]]
    identifier = &quot;posts&quot;
    name       = &quot;Posts&quot;
    url        = &quot;/posts&quot;</code></pre><p>All done! &#x1F389;&#x1F389;&#x1F389;</p>]]></content:encoded></item><item><title><![CDATA[Hello Caddy]]></title><description><![CDATA[<p>Since starting self-hosting back in 2017, I&apos;ve always used apache2 since it&apos;s the first webserver I came across. Over time adding more services and managing separate vhost config is a bit tiresome.</p><p>Enters Caddy. It&apos;s very simple to set up and configure. Some services</p>]]></description><link>https://blog.karnwong.me/hello-caddy/</link><guid isPermaLink="false">60448c903b0e1d0001e528da</guid><category><![CDATA[infra]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sun, 07 Mar 2021 08:32:19 GMT</pubDate><content:encoded><![CDATA[<p>Since starting self-hosting back in 2017, I&apos;ve always used apache2 since it&apos;s the first webserver I came across. Over time adding more services and managing separate vhost config is a bit tiresome.</p><p>Enters Caddy. It&apos;s very simple to set up and configure. Some services where I have trouble setting up in apache2 do not need extra config at all, even TLS is set up by default. Starting from Caddy2 it works with CNAME by default without extra setups.</p><p>You can set it up using a Caddy docker container, but some containers I use also expose port 443, so I have to install Caddy natively instead. </p><p>For multiple sites config setup:</p><pre><code class="language-caddy"># /etc/caddy/Caddyfile

SUBDOMAIN1.DOMAIN.com {
    reverse_proxy 127.0.0.1:${PORT}
}
SUBDOMAIN2.DOMAIN.com {
    reverse_proxy 127.0.0.1:${PORT}
}</code></pre><p>For basic authentication, it&apos;s very, very simple (to the point I regret time researching it in apache2):</p><pre><code class="language-caddy"># generate password hash
caddy hash-password --algorithm bcrypt

# add basicauth to Caddyfile
SUBDOMAIN1.DOMAIN.com {
    basicauth * {
        ${USERNAME} ${CADDY_PASSWORD_HASH}
    }
    reverse_proxy 127.0.0.1:${PORT}
}</code></pre><p>And run <code>systemctl reload caddy</code>. You&apos;re all set!</p>]]></content:encoded></item><item><title><![CDATA[Password auth with apache2 reverse-proxy]]></title><description><![CDATA[<p>EDIT: see <a href="https://blog.karnwong.me/hello-caddy/">https://blog.karnwong.me/hello-caddy/</a> for Caddy, also easier to set up too.</p><p>Sometimes you found an interesting project to self-hosted, but it doesn&apos;t have password authentication built-in. Luckily, we need to reverse-proxy them anyway and apache2/ nginx / httpd happen to provide password auth with reverse-proxy</p>]]></description><link>https://blog.karnwong.me/setting-up-password-auth-with-apache2-reverse-proxy/</link><guid isPermaLink="false">60335d453b0e1d0001e5289f</guid><category><![CDATA[infra]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Mon, 22 Feb 2021 07:38:07 GMT</pubDate><content:encoded><![CDATA[<p>EDIT: see <a href="https://blog.karnwong.me/hello-caddy/">https://blog.karnwong.me/hello-caddy/</a> for Caddy, also easier to set up too.</p><p>Sometimes you found an interesting project to self-hosted, but it doesn&apos;t have password authentication built-in. Luckily, we need to reverse-proxy them anyway and apache2/ nginx / httpd happen to provide password auth with reverse-proxy by default.</p><p>To set up password auth with apache2 via reverse-proxy:</p><ol><li><code>echo &quot;${PASSWORD}&quot; | htpasswd -c -i /etc/apache2/.htpasswd ${USER}</code> on your host machine which has apache2 installed.</li><li>create a vhost config:</li></ol><!--kg-card-begin: markdown--><pre><code class="language-xml">&lt;VirtualHost *:80&gt;
    ProxyPreserveHost On

    ProxyPass / http://localhost:${EXPOSED_CONTAINER_PORT}/
    ProxyPassReverse / http://localhost:${EXPOSED_CONTAINER_PORT}/

    ServerName ${YOUR_DOMAIN}

    &lt;Proxy *&gt;
        Order deny,allow
        Allow from all
        Authtype Basic
        Authname &quot;Password Required&quot;
        AuthUserFile /etc/apache2/.htpasswd
        Require valid-user
    &lt;/Proxy&gt;
&lt;/virtualhost&gt;
</code></pre>
<!--kg-card-end: markdown--><p>That&apos;s it!</p>]]></content:encoded></item><item><title><![CDATA[Buying tea when you have Celiac]]></title><description><![CDATA[<p>Might come as a surprise to some of you, but tea <em>can</em> contain gluten from additives &amp; cross-contamination, in which barley or malt is added for flavorings. Teavana is known for adding such additives in their tea (and I got glutented from it one time).</p><p>Say, I&apos;m interested</p>]]></description><link>https://blog.karnwong.me/buying-tea-when-you-have-celiac/</link><guid isPermaLink="false">6028b0d63b0e1d0001e5285e</guid><category><![CDATA[celiac]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sun, 14 Feb 2021 05:20:54 GMT</pubDate><content:encoded><![CDATA[<p>Might come as a surprise to some of you, but tea <em>can</em> contain gluten from additives &amp; cross-contamination, in which barley or malt is added for flavorings. Teavana is known for adding such additives in their tea (and I got glutented from it one time).</p><p>Say, I&apos;m interested in some tea. I first need to look up its country of origin, because that tells me about how good the food labeling laws is. If it&apos;s from Australia / New Zealand, this means the label will always state gluten. If it&apos;s from EU (not pan-Europe) it&apos;s also good, except they allow products with &lt; 20 PPM to be labeled gluten-free. But this is still good. If it&apos;s from USA, a lot of scrutiny is called because the US FDA does not enforce gluten labeling, only wheat is required. But not all gluten-y stuff is wheat :/</p><p>In this case, I&apos;m interested in Turkish tea. So I asked a Turkish friend to do some digging for me on the manufacturer&apos;s website, since I couldn&apos;t find the info in English. He reported that he didn&apos;t find any info about gluten on the website, but he send me a photo of the package where the ingredients are listed. I spotted German on the labels, this is a good sign because it means this product is also sold in Germany, and food labels have to comply with the country it&apos;s being sold in too*.</p><p>That&apos;s pretty much end of story. Ordered the tea and brew it. Tastes very good!</p><figure class="kg-card kg-image-card"><img src="https://blog.karnwong.me/content/images/2021/02/image.png" class="kg-image" alt loading="lazy" width="900" height="707" srcset="https://blog.karnwong.me/content/images/size/w600/2021/02/image.png 600w, https://blog.karnwong.me/content/images/2021/02/image.png 900w" sizes="(min-width: 720px) 720px"></figure><p>*Thank you a local Thai QA who told me about this &#x1F64F;</p>]]></content:encoded></item><item><title><![CDATA[Workarounds for archiving large shapefile in data lake]]></title><description><![CDATA[<p>If you work with spatial data, chances are you are familiar with <code>shapefile</code>, a file format for viewing / editing spatial data.</p><p>Essentially, shapefile is just a tabular data like <code>csv</code>, but it does thingamajig with <code>geometry</code> data type, where any gis tools like <code>qgis</code> or <code>arcgis</code> can understand right away.</p>]]></description><link>https://blog.karnwong.me/workarounds-for-archiving-large-shapefile-in-data-lake/</link><guid isPermaLink="false">6016e5f18f9b6a00014fe01f</guid><category><![CDATA[data engineering]]></category><category><![CDATA[gis]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Sun, 31 Jan 2021 17:40:53 GMT</pubDate><content:encoded><![CDATA[<p>If you work with spatial data, chances are you are familiar with <code>shapefile</code>, a file format for viewing / editing spatial data.</p><p>Essentially, shapefile is just a tabular data like <code>csv</code>, but it does thingamajig with <code>geometry</code> data type, where any gis tools like <code>qgis</code> or <code>arcgis</code> can understand right away. If you have a csv file with geometry column in <code>wkt</code> format (something like <code>POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))</code>), you&apos;ll have to specify which column is to be used for geometry.</p><p>If you want to store shapefile in data lake, it&apos;s best to store it as parquet or any format you normally use, since it&apos;s faster to read and filter. For comparison, parsing a 5GB+ shapefile and filter takes <em>longer</em> than reading a gzipped json, filter, and export to shapefile.</p><p>Normally I would use <code>geopandas</code> to read spatial data and convert it to pandas dataframe, then send it to spark. But since the shapefile is very large, it takes forever to read in geopandas. This tells me that there is a parsing bottleneck going on. And geopandas can&apos;t read shapefile with multiple geometry types (this shouldn&apos;t happen, but sometimes during editing, clipping this here and there can cause invalid geometry).</p><p>Qgis has a tool to fix invalid geometries, so I tried exporting shapefile to csv, but qgis went OOM. But both qgis and geopandas use <code>gdal</code> for backend, and it has a CLI interface, so I look up how to export shapefile to tsv (<code>tab</code> as a seperator makes it faster to parse since it rarely occurs).</p><p>Now things work perfectly. As a bonus, gdal also skip invalid geometries by default (unlike in geopandas where it will throw an error and there&apos;s no way to ignore it and tell the parser to keep going).</p><p>At this point I have a nice tsv file, and reading &amp; archiving via spark is now a breeze. Yay!</p><h1 id="takeaway">Takeaway</h1><ul><li>If it takes too long to read, maybe it&apos;s a parsing bottleneck. Find a way to convert it to another format so it&apos;s eaiser to parse.</li><li>Sometimes your initial tools of choice might have some quirks. In most cases there will be similar tools out there that can workaround the issues. (In this case, use gdal to convert to csv in lieu of geopandas because gpd can&apos;t work with invalid geometries &amp; takes longer to read compared to feeding spark a straight csv/tsv).</li></ul>]]></content:encoded></item><item><title><![CDATA[Mongodb export woes]]></title><description><![CDATA[<p>There&apos;s a task where I need to export 4M+ records out of mongodb, total uncompressed size is <s>17GB+</s> 26GB</p><h1 id="export-methods">export methods</h1><h3 id="mongoexport">mongoexport</h3><p>The recommended way to export is using <code>mongoexport</code> utility, but you have to specify the output attributes, which doesn&apos;t work for me because the</p>]]></description><link>https://blog.karnwong.me/mongodb-export-woes/</link><guid isPermaLink="false">6010eca30ae88a00013bb800</guid><category><![CDATA[data engineering]]></category><dc:creator><![CDATA[Karn Wong]]></dc:creator><pubDate>Wed, 27 Jan 2021 04:51:41 GMT</pubDate><content:encoded><![CDATA[<p>There&apos;s a task where I need to export 4M+ records out of mongodb, total uncompressed size is <s>17GB+</s> 26GB</p><h1 id="export-methods">export methods</h1><h3 id="mongoexport">mongoexport</h3><p>The recommended way to export is using <code>mongoexport</code> utility, but you have to specify the output attributes, which doesn&apos;t work for me because the schema from older set of records are less than the newer set</p><h3 id="diy-python-script">DIY python script</h3><h4 id="the-vanilla-way">the vanilla way</h4><p>But you can interact with mongodb from python, and if you read from it it&apos;ll return a dict, which is perfect for this because you don&apos;t have to specify the required attributes beforehand. So what I do is:</p><pre><code class="language-python">cursor = collection.find({})
total_records = collection.estimated_document_count()

with open(filename, &apos;w&apos;) as f:
    for i in tqdm(cursor, total=total_records):
        f.write(json.dumps(i, default=myconverter, ensure_ascii=False))
        f.write(&apos;\n&apos;)</code></pre><p>The cons for this solution is it needs a lot of hdd space since it&apos;s uncompressed. But it <strong>works best if you need to export a collection with mismatched schema</strong>.</p><h4 id="the-incremental-export-way">the incremental export way</h4><p>You can also incrementally export your collection from mongodb using <code>.skip($START_INDEX).limit($INCREMENT_SIZE)</code> , but it <strong>performs worse than the vanilla way</strong>, since what mongodb does is just iterating through everything all over again to get to your specified <code>start:end</code> index.</p><h2 id="performance-comparison">Performance comparison</h2><p>On my local machine (&lt;10 MB/s transfer speed) I could export a collection with around 4.5M records within 1 hour, but on a VPS with incremental export it takes 9 hours and counting.</p><h2 id="takeaway">Takeaway</h2><p>Please do not store a large dataset in mongodb where you need to dump everything out, especially if you use it as a raw data source. It&apos;s fine if you store prepped output for API to be queried via <code>_id</code> (primary key).</p><p></p>]]></content:encoded></item></channel></rss>